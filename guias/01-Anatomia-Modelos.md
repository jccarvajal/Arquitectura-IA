## Bloque 1: Fundamentos T√©cnicos (C√≥mo funciona)

### Gu√≠a 01: Anatom√≠a y Entrenamiento de Modelos Generativos
### La capa invisible que determina el Riesgo y la Utilidad

Para decidir, dise√±ar y gobernar adecuadamente sobre Inteligencia Artificial, es imperativo desmontar la noci√≥n de "caja negra". Los modelos generativos modernos (como GPT-4, Claude 3.5 o Llama 3) no son bases de conocimiento ni sistemas de razonamiento l√≥gico en sentido humano: son **motores probabil√≠sticos de predicci√≥n**, moldeados a trav√©s de m√∫ltiples fases de entrenamiento.

Este anexo describe el ciclo de vida t√©cnico que transforma terabytes de texto crudo en un asistente capaz de seguir instrucciones. El objetivo es que el arquitecto decida en funci√≥n de **criterio de ingenier√≠a**, no de intuici√≥n ni del *hype* del mercado.

---

## 1. El Motor Base: Arquitectura Transformer

La generaci√≥n actual se sustenta en la arquitectura **Transformer** (Vaswani et al., 2017). Su innovaci√≥n central es el procesamiento paralelo y el **Mecanismo de Atenci√≥n**, que asigna "pesos" de relevancia entre partes distantes de una secuencia.

### A. Tokenizaci√≥n
Los modelos no procesan palabras, sino **tokens** (fragmentos num√©ricos).
* **Implicancia Econ√≥mica:** Un tokenizador ineficiente ‚Äîespecialmente en modelos angloc√©ntricos aplicados al espa√±ol‚Äî aumenta el costo real de inferencia.
* **Implicancia T√©cnica:** M√°s tokens para expresar la misma idea implica mayor latencia y mayor superficie para alucinaciones.

### B. Ventana de Contexto y Atenci√≥n
La atenci√≥n permite al modelo relacionar conceptos distantes para mantener coherencia narrativa y l√≥gica.
* **Implicancia de Dise√±o:** La calidad del razonamiento (lo que algunos llaman "Sistema 2") est√° limitada por la fidelidad del mecanismo de atenci√≥n y por el tama√±o de la ventana de contexto. Contextos saturados degradan la capacidad instruccional (*Lost in the Middle phenomenon*).

---

## 2. Fase 1: Pre-Entrenamiento (Pre-training)
**El nacimiento del "Modelo Base"**

> **Nota terminol√≥gica:** En la industria, el "entrenamiento principal" del modelo se denomina **Pre-Entrenamiento**. Aunque el nombre parezca preliminar, esta es la fase donde realmente se construyen los pesos fundamentales. Las etapas posteriores (SFT, RLHF, RLAIF) no reemplazan esta base; la especializan.

Es la fase de mayor inversi√≥n (meses de c√≥mputo en miles de GPUs). El modelo aprende por autosupervisi√≥n: predecir el siguiente token o completar textos masivos.

* **Resultado:** Un **Modelo Base** (Foundation Model).
* **Propiedad Clave:** Posee un amplio conocimiento latente, pero carece de capacidad estructurada de seguir instrucciones.

> **‚ö†Ô∏è Riesgo de Gobernanza:** Implementar un Modelo Base creyendo que es un asistente produce incoherencias, sesgos sin filtrar y vulnerabilidad total a inyecci√≥n de prompts.

---

## 3. Fase 2: Post-Entrenamiento (Post-training)
**La creaci√≥n del Asistente**

Esta fase convierte al motor estad√≠stico en un sistema √∫til y seguro. Se divide en capas conductuales y normativas.

### A. SFT (Supervised Fine-Tuning)
Entrenamiento con pares curados **Instrucci√≥n -> Respuesta** escritos por humanos.
* **Funci√≥n:** Ense√±ar al modelo a estructurar di√°logos, seguir pasos y adoptar un tono √∫til.
* **Riesgo:** **Alucinaci√≥n Confiada**. El modelo puede dar respuestas err√≥neas con un tono autoritativo.

### B. RLHF (Reinforcement Learning from Human Feedback)
Capa cl√°sica de alineaci√≥n.
1.  Humanos ordenan respuestas.
2.  Se entrena un "Modelo de Recompensa".
3.  El LLM se ajusta para maximizar esa recompensa.
* **Limitaci√≥n:** Puede inducir **Negative Refusal** (rechazo excesivo por sobreprotecci√≥n ideol√≥gica o de seguridad).

### C. RLAIF (Constitutional AI / AI Feedback)
Alineaci√≥n escalable utilizando **IA supervisando IA**.
* **Mecanismo:** Una "Constituci√≥n" de reglas expl√≠citas gu√≠a el juicio del modelo supervisor.
* **Ventaja:** Mayor consistencia y menor variabilidad humana.

---

## 4. Resumen Estrat√©gico: Las Capas del Modelo

| Capa | Naturaleza | Funci√≥n Real | Riesgo Principal |
| :--- | :--- | :--- | :--- |
| **Modelo Base** | Estad√≠stica | Predecir tokens | Incoherencia y falta de control. |
| **SFT** | Conductual | Seguir instrucciones | Alucinaci√≥n confiada. |
| **RLHF/RLAIF** | Normativa | Alinear con valores | Rechazos falsos positivos. |

---

## 5. De la Teor√≠a a la Auditor√≠a: Documentaci√≥n

Para aplicar GRC, exige la documentaci√≥n correcta para cada fase:

### üîç Model Card (Ficha del Motor)
Documenta la **Fase 1 (Pre-entrenamiento)**.
* **Qu√© buscar:** Fecha de corte (*cut-off date*), arquitectura t√©cnica, benchmarks de razonamiento y tama√±o de contexto.
* **Uso:** Viabilidad t√©cnica y costo de infraestructura.

### üõ°Ô∏è System Card (Ficha de Seguridad)
Documenta la **Fase 2 (Post-entrenamiento)**.
* **Qu√© buscar:** Metodolog√≠a de alineaci√≥n, resultados de *Red Teaming*, tasas de rechazo y mitigaci√≥n de sesgos.
* **Uso:** Cumplimiento normativo, √©tica y seguridad operativa.

---

## 6. Herramienta Pr√°ctica: Checklist de Auditor√≠a

Utilice este cuestionario para evaluar modelos en contextos corporativos o de contrataci√≥n p√∫blica, contrastando la informaci√≥n de la *Model Card* y la *System Card*.

### I. Auditor√≠a del Modelo Base (Model Card)
*Evaluaci√≥n de capacidades fundamentales y viabilidad t√©cnica.*

| Pregunta de Control | Evidencia Esperada | Riesgo Asociado | Acci√≥n Mitigadora |
| :--- | :--- | :--- | :--- |
| **¬øCu√°l es la fecha de corte del conocimiento?** | Fecha expl√≠cita. | Respuestas obsoletas; decisiones incorrectas. | Restringir dominios cr√≠ticos o usar RAG (Retrieval). |
| **¬øQu√© arquitectura utiliza y cu√°ntos par√°metros tiene?** | Descripci√≥n t√©cnica (ej. Dense vs MoE). | Dificultad para estimar costos y latencia. | Solicitar transparencia m√≠nima o benchmarks de inferencia. |
| **¬øCu√°l es el tama√±o de la ventana de contexto?** | Valor en tokens (ej. 128k). | P√©rdida de informaci√≥n en tareas largas ("Olvido"). | Fragmentar tareas o usar herramientas de resumen. |
| **¬øCu√°les son los resultados en benchmarks est√°ndar?** | MMLU, HumanEval. | Modelo insuficiente para tareas de razonamiento. | Escalar a un modelo m√°s avanzado (Frontier Model). |

### II. Auditor√≠a del Post-Entrenamiento (System Card)
*Evaluaci√≥n de alineaci√≥n, seguridad y comportamiento.*

| Pregunta de Control | Evidencia Esperada | Riesgo Asociado | Acci√≥n Mitigadora |
| :--- | :--- | :--- | :--- |
| **¬øQu√© metodolog√≠a SFT se us√≥?** | Descripci√≥n del dataset. | El modelo no sigue instrucciones de formato. | Afinar *System Prompts* o realizar SFT adicional. |
| **¬øExisten resultados de RLHF o RLAIF?** | Detalles del *Reward Model*. | Respuestas peligrosas, t√≥xicas o ideol√≥gicas. | Aplicar filtros externos (Guardrails/LOSA). |
| **¬øSe documentaron pruebas de Red Teaming?** | Casu√≠stica de ataques. | Fugas de informaci√≥n, *jailbreaks* exitosos. | Implementar capa adicional de seguridad de entrada/salida. |
| **¬øCu√°les son las tasas de rechazo (*Refusal Rates*)?** | M√©tricas de rechazo. | *Negative Refusal*; bloqueo injustificado de tareas. | Ajustar el modelo o cambiar proveedor si es muy restrictivo. |

### III. Dictamen de Auditor√≠a (Plantilla)

**Fecha de Evaluaci√≥n:** `DD/MM/AAAA`
**Modelo Evaluado:** `[Nombre del Modelo y Versi√≥n]`

* **Conclusi√≥n T√©cnica:** `[Viable / No Viable]`
* **Conclusi√≥n Normativa:** `[Cumple / No Cumple]`

**Recomendaci√≥n Final:**
[ ] APROBAR | [ ] APROBAR CON CONDICIONES | [ ] NO APROBAR

---
<div style="display: flex; justify-content: space-between; font-size: 0.9em; padding-top: 10px;">
  <div>
    <a href="../ideas-centrales.md">¬´ Ideas Centrales</a>
  </div>
  <div>
    <a href="../">Volver al √çndice</a>
  </div>
  <div>
    <a href="./02-Ingenieria-Prompts.md">Gu√≠a 02 ¬ª</a>
  </div>
</div>