## Bloque 1: Fundamentos T√©cnicos (C√≥mo funciona)

### Gu√≠a 01: Anatom√≠a y Entrenamiento de Modelos Generativos

Subt√≠tulo: La capa invisible que determina el Riesgo y la Utilidad

### Introducci√≥n: De la "Caja Negra" al Plano de Ingenier√≠a

Para la mayor√≠a de los usuarios, una IA es una **"Caja Negra"**: un sistema opaco donde entra texto y sale magia. No sabemos qu√© ocurre dentro, por lo que tendemos a atribuirle cualidades humanas ("la IA piensa", "la IA quiere").

Este pensamiento m√°gico es peligroso para un Arquitecto. Si no entiendes los l√≠mites f√≠sicos de la m√°quina, no puedes gobernarla. En esta gu√≠a, vamos a "abrir la caja". Vamos a desmontar el motor para entender sus componentes mec√°nicos: el Ciclo de Entrenamiento (c√≥mo aprende), la Inferencia (c√≥mo act√∫a) y los Par√°metros (su capacidad). Dejaremos de ver magia y empezaremos a ver ingenier√≠a.

Los modelos generativos modernos (como Gemini 3, GPT-5 o Llama 4) no son bases de conocimiento ni sistemas de razonamiento l√≥gico en sentido humano: son **motores probabil√≠sticos de predicci√≥n**, moldeados a trav√©s de m√∫ltiples fases de entrenamiento.

Este anexo describe el ciclo de vida t√©cnico que transforma terabytes de texto crudo en un asistente capaz de seguir instrucciones. El objetivo es que el arquitecto decida en funci√≥n de **criterio de ingenier√≠a**, no de intuici√≥n ni del *hype* del mercado.

---

### Parte 1: El Mapa del Territorio: IA vs. ML vs. DL

Antes de inspeccionar el motor, debemos ubicarlo en el mapa. Es com√∫n usar los t√©rminos "IA", "Machine Learning" y "Deep Learning" indistintamente, pero son conceptos jer√°rquicos, como mu√±ecas rusas (Matrioskas).

* **Inteligencia Artificial (IA):** Es el concepto general. Se refiere a cualquier t√©cnica que permita a las computadoras imitar el comportamiento humano (l√≥gica, reglas si-entonces, √°rboles de decisi√≥n).
* **Machine Learning (ML - Aprendizaje Autom√°tico):** Es un subconjunto de la IA. Aqu√≠, la m√°quina no se programa con reglas expl√≠citas, sino que "aprende" patrones a partir de datos estad√≠sticos para hacer predicciones.
* **Deep Learning (DL - Aprendizaje Profundo):** Es un subconjunto del ML inspirado en la biolog√≠a. Utiliza *Redes Neuronales Artificiales* con muchas capas ("profundas") para procesar datos complejos. Aqu√≠ es donde viven los LLMs (Large Language Models).
* **IA Generativa (GenAI):** Es la frontera actual del Deep Learning. A diferencia de la IA tradicional que analiza o clasifica (ej. filtro de spam), la GenAI puede crear nuevo contenido (texto, c√≥digo, im√°genes) que no exist√≠a previamente, bas√°ndose en los patrones aprendidos.

**Implicancia para el Arquitecto:** Cuando hablamos de "Arquitectura de IA" en esta obra, nos referimos espec√≠ficamente a **Deep Learning Generativo**. Esto implica que no trabajamos con sistemas deterministas (reglas fijas), sino con sistemas probabil√≠sticos (predicciones creativas pero falibles).

---

### Parte 2: El Motor Base: Arquitectura Transformer

La generaci√≥n actual se sustenta en la arquitectura **Transformer** (Vaswani et al., 2017). Su innovaci√≥n central es el procesamiento paralelo y el **Mecanismo de Atenci√≥n**, que asigna "pesos" de relevancia entre partes distantes de una secuencia.

#### A. Tokenizaci√≥n
Los modelos no procesan palabras, sino **tokens** (fragmentos num√©ricos).

* **Implicancia Econ√≥mica:** Un tokenizador ineficiente, especialmente en modelos angloc√©ntricos aplicados al espa√±ol, aumenta el costo real de inferencia.
* **Implicancia T√©cnica:** M√°s tokens para expresar la misma idea implica mayor latencia y mayor superficie para alucinaciones.

#### B. Ventana de Contexto y Atenci√≥n
La atenci√≥n permite al modelo relacionar conceptos distantes para mantener coherencia narrativa y l√≥gica.

* **Implicancia de Dise√±o:** La calidad del razonamiento (lo que en esta obra definimos como ‚ÄúSistema 2‚Äù) est√° limitada por la fidelidad del mecanismo de atenci√≥n y por el tama√±o de la ventana de contexto. Contextos saturados degradan la capacidad instruccional (*Lost in the Middle phenomenon*).

---

### Parte 3: Fase 1: Pre-Entrenamiento (Pre-training)
**El nacimiento del "Modelo Base"**

> **Nota terminol√≥gica:** En la industria, el "entrenamiento principal" del modelo se denomina **Pre-Entrenamiento**. Aunque el nombre parezca preliminar, esta es la fase donde realmente se construyen los pesos fundamentales. Las etapas posteriores (SFT, RLHF, RLAIF) no reemplazan esta base; la especializan.

Es la fase de mayor inversi√≥n (meses de c√≥mputo en miles de GPUs). El modelo aprende por autosupervisi√≥n: predecir el siguiente token o completar textos masivos.

* **Resultado:** Un **Modelo Base** (Foundation Model).
* **Propiedad Clave:** Posee un amplio conocimiento latente, pero carece de capacidad estructurada de seguir instrucciones.

> **‚ö†Ô∏è Riesgo de Gobernanza:** Implementar un Modelo Base creyendo que es un asistente produce incoherencias, sesgos sin filtrar y vulnerabilidad total a inyecci√≥n de prompts.

---

### Parte 4: Fase 2: Post-Entrenamiento (Post-training)

**La creaci√≥n del Asistente:** Esta fase convierte al motor estad√≠stico en un sistema √∫til y seguro. Se divide en capas conductuales y normativas.

#### A. SFT (Supervised Fine-Tuning) - El Entrenamiento Conductual
En esta etapa, el modelo deja de ser un simple predictor de texto (que solo quiere completar frases) para convertirse en un asistente dialogante. Se le alimenta con miles de ejemplos de alta calidad en formato **(Instrucci√≥n, Respuesta Ideal)** escritos por humanos expertos. Es como enviar al modelo a una "escuela de modales" donde aprende el formato de pregunta-respuesta.

* **Funci√≥n:** Ense√±ar al modelo a estructurar di√°logos coherentes, seguir instrucciones complejas paso a paso y adoptar un tono de servicio √∫til.
* **Riesgo:** **Alucinaci√≥n Confiada**. Como el modelo aprende la *forma* de una respuesta correcta (el estilo seguro y profesional) antes que el *contenido* factual, puede entregar informaci√≥n falsa con un tono de autoridad total, "impostando" competencia.

#### B. RLHF (Reinforcement Learning from Human Feedback) - El Ajuste de Preferencias
Esta es la capa cl√°sica de alineaci√≥n √©tica y de seguridad. Dado que es imposible escribir una regla para cada situaci√≥n social posible, se utiliza un sistema de "premios y castigos" basado en la preferencia humana.

1.  **Comparaci√≥n:** Los humanos no escriben respuestas, sino que **ordenan** dos respuestas generadas por la IA (Opci√≥n A vs. Opci√≥n B) seg√∫n cu√°l es mejor (m√°s segura, m√°s √∫til, menos t√≥xica).
2.  **Modelo de Recompensa:** Esos datos entrenan a un segundo modelo (el *Reward Model*) que aprende a predecir qu√© preferir√≠a un humano.
3.  **Optimizaci√≥n:** El LLM principal juega millones de partidas contra este Modelo de Recompensa, ajustando sus pesos para maximizar su puntaje de aprobaci√≥n.
* **Limitaci√≥n:** Puede inducir **Negative Refusal** (Rechazo Negativo). Si el modelo es castigado excesivamente por temas sensibles durante el entrenamiento, se vuelve paranoico y empieza a rechazar solicitudes inocuas (ej: rechazar "c√≥mo matar un proceso en Linux" por considerarlo violento).

#### C. RLAIF (Constitutional AI / AI Feedback) - La Alineaci√≥n Escalable
El cuello de botella del RLHF son los humanos: son lentos, caros, inconsistentes y se cansan. RLAIF soluciona esto reemplazando al evaluador humano por otra IA.

* **Mecanismo:** En lugar de preferencias subjetivas de una multitud, se utiliza una **"Constituci√≥n"** (un set de principios expl√≠citos, ej: "Elige la respuesta que sea m√°s √∫til y menos da√±ina"). Un Modelo Supervisor usa esta constituci√≥n para evaluar y entrenar al Modelo Principal a una velocidad y escala imposible para humanos.
* **Ventaja:** Mayor consistencia normativa (la IA no se cansa ni tiene d√≠as malos) y transparencia (las reglas est√°n escritas en la Constituci√≥n, no en la mente subjetiva de miles de contratistas).

---

### Parte 5: Resumen Estrat√©gico: Las Capas del Modelo

**La Arquitectura en Capas:** Para efectos de auditor√≠a y gesti√≥n de riesgos, visualice el modelo final no como un bloque monol√≠tico, sino como una "lasa√±a" de tres capas funcionales. Cada capa aporta una capacidad espec√≠fica, pero tambi√©n introduce un riesgo inherente. El Arquitecto debe entender que un fallo en la capa inferior (estad√≠stica) no puede ser arreglado completamente en la capa superior (normativa); los cimientos defectuosos comprometen toda la estructura.

| Capa | Naturaleza | Funci√≥n Real | Riesgo Principal |
| :--- | :--- | :--- | :--- |
| **Modelo Base** | Estad√≠stica | Predecir tokens | Incoherencia y falta de control. |
| **SFT** | Conductual | Seguir instrucciones | Alucinaci√≥n confiada. |
| **RLHF/RLAIF** | Normativa | Alinear con valores | Rechazos falsos positivos. |

---

### Parte 6: De la Teor√≠a a la Auditor√≠a: Documentaci√≥n

El "Arquitecto de IA" no opera bas√°ndose en comunicados de prensa o marketing. Opera bas√°ndose en **evidencia t√©cnica documentada**.

La industria ha estandarizado la transparencia en dos documentos clave. Para realizar una auditor√≠a completa de GRC, usted debe exigir y analizar ambos.

**La F√≥rmula de Auditor√≠a:**
> **Viabilidad T√©cnica (Model Card) + Seguridad Operativa (System Card) = Aprobaci√≥n de Despliegue**

#### üîç A. Model Card (Ficha del Motor)
**Documenta la Fase 1 (Pre-entrenamiento).**
Es el "Manual de Especificaciones T√©cnicas" del motor. Nos dice qu√© tan potente es el modelo en bruto, antes de ser alineado para seguridad.

* **Objetivo:** Evaluar si el modelo tiene la capacidad intelectual y f√≠sica para la tarea.
* **Datos Cr√≠ticos que Contiene:**
    * **Arquitectura y Par√°metros:** El tama√±o real del modelo (ej. 70B, 8x22B MoE) que determina el costo de hosting.
    * **Fecha de Corte (Cut-off date):** El d√≠a exacto en que el modelo "dej√≥ de aprender" del mundo. Vital para saber si conoce leyes o eventos recientes.
    * **Ventana de Contexto:** La capacidad de memoria a corto plazo (ej. 128k tokens).
    * **Benchmarks de Razonamiento:** Puntajes en pruebas estandarizadas (MMLU, HumanEval) que demuestran su capacidad l√≥gica y de codificaci√≥n.
* **Uso en GRC:** Determina la **Viabilidad T√©cnica** y el **Costo de Infraestructura**.

#### üõ°Ô∏è B. System Card (Ficha de Seguridad)
**Documenta la Fase 2 (Post-entrenamiento).**
Es el "Informe de Seguridad y Riesgos". Nos dice c√≥mo se comporta el modelo ante usuarios adversarios y qu√© controles tiene activados.

* **Objetivo:** Evaluar si es seguro exponer este modelo a empleados o ciudadanos.
* **Datos Cr√≠ticos que Contiene:**
    * **Metodolog√≠a de Alineaci√≥n:** Detalles sobre c√≥mo se aplic√≥ RLHF o RLAIF para filtrar toxicidad.
    * **Resultados de Red Teaming:** Reportes de ataques simulados (ej. intentos de crear armas biol√≥gicas o ciberataques) y c√≥mo el modelo se defendi√≥.
    * **Tasas de Rechazo (Refusal Rates):** Estad√≠sticas sobre cu√°ntas veces el modelo se niega a responder (√∫til para detectar si es "demasiado puritano").
    * **Mitigaci√≥n de Sesgos:** Pruebas espec√≠ficas sobre estereotipos de g√©nero, raza o cultura.
* **Uso en GRC:** Determina el **Cumplimiento Normativo**, la **√âtica** y la **Seguridad Operativa**.

---

### Parte 7: Herramienta Pr√°ctica: Checklist de Auditor√≠a

A continuaci√≥n, se presentan las tablas de control para evaluar modelos en contextos corporativos o de contrataci√≥n p√∫blica. Estas listas de verificaci√≥n permiten contrastar las promesas comerciales con la realidad t√©cnica descrita en la *Model Card* y la *System Card*.

#### I. Auditor√≠a del Modelo Base (Model Card)

**Evaluaci√≥n de capacidades fundamentales y viabilidad t√©cnica:** Esta secci√≥n eval√∫a la *"Viabilidad T√©cnica"*. 
Buscamos responder: *¬øTiene este motor la capacidad f√≠sica y el conocimiento necesario para realizar la tarea?*

| Pregunta de Control | Evidencia Esperada | Riesgo Asociado | Acci√≥n Mitigadora |
| :--- | :--- | :--- | :--- |
| **¬øCu√°l es la fecha de corte del conocimiento?** | Fecha expl√≠cita. | Respuestas obsoletas; decisiones incorrectas. | Restringir dominios cr√≠ticos o usar RAG (Retrieval). |
| **¬øQu√© arquitectura utiliza y cu√°ntos par√°metros tiene?** | Descripci√≥n t√©cnica (ej. Dense vs MoE). | Dificultad para estimar costos y latencia. | Solicitar transparencia m√≠nima o benchmarks de inferencia. |
| **¬øCu√°l es el tama√±o de la ventana de contexto?** | Valor en tokens (ej. 128k). | P√©rdida de informaci√≥n en tareas largas ("Olvido"). | Fragmentar tareas o usar herramientas de resumen. |
| **¬øCu√°les son los resultados en benchmarks est√°ndar?** | MMLU, HumanEval. | Modelo insuficiente para tareas de razonamiento. | Escalar a un modelo m√°s avanzado (Frontier Model). |

#### II. Auditor√≠a del Post-Entrenamiento (System Card)

**Evaluaci√≥n de alineaci√≥n, seguridad y comportamiento:** Esta secci√≥n eval√∫a la *"Seguridad y Alineaci√≥n"*. 
Buscamos responder: *¬øEs seguro desplegar este modelo ante usuarios o empleados? ¬øCumple con nuestras normas de gobernanza?*

| Pregunta de Control | Evidencia Esperada | Riesgo Asociado | Acci√≥n Mitigadora |
| :--- | :--- | :--- | :--- |
| **¬øQu√© metodolog√≠a SFT se us√≥?** | Descripci√≥n del dataset. | El modelo no sigue instrucciones de formato. | Afinar *System Prompts* o realizar SFT adicional. |
| **¬øExisten resultados de RLHF o RLAIF?** | Detalles del *Reward Model*. | Respuestas peligrosas, t√≥xicas o ideol√≥gicas. | Aplicar filtros externos (Guardrails/LOSA). |
| **¬øSe documentaron pruebas de Red Teaming?** | Casu√≠stica de ataques. | Fugas de informaci√≥n, *jailbreaks* exitosos. | Implementar capa adicional de seguridad de entrada/salida. |
| **¬øCu√°les son las tasas de rechazo (*Refusal Rates*)?** | M√©tricas de rechazo. | *Negative Refusal*; bloqueo injustificado de tareas. | Ajustar el modelo o cambiar proveedor si es muy restrictivo. |

#### III. Dictamen de Auditor√≠a (Plantilla)

**Cierre del Proceso:** Para finalizar la auditor√≠a, no basta con listar hallazgos; es imperativo tomar una decisi√≥n ejecutiva documentada. Esta plantilla consolida la viabilidad t√©cnica (proveniente de la *Model Card*) y la alineaci√≥n de seguridad (proveniente de la *System Card*) en un dictamen formal. √ösela como el "sello de autorizaci√≥n" indispensable antes de que cualquier modelo toque infraestructura productiva o datos reales.

**Fecha de Evaluaci√≥n:** `DD/MM/AAAA`
**Modelo Evaluado:** `[Nombre del Modelo y Versi√≥n]`

* **Conclusi√≥n T√©cnica:** `[Viable / No Viable]`
* **Conclusi√≥n Normativa:** `[Cumple / No Cumple]`

**Recomendaci√≥n Final:** `[ ] APROBAR | [ ] APROBAR CON CONDICIONES | [ ] NO APROBAR`

---

### Conclusi√≥n: El Fin del Antropomorfismo

Hemos abierto el chasis y hemos visto lo que hay dentro. No hay un "esp√≠ritu" en la m√°quina; hay matrices de n√∫meros, operaciones de punto flotante en una GPU y un ciclo de predicci√≥n estad√≠stica.

Comprender la anatom√≠a del modelo (Entrenamiento vs. Inferencia, Par√°metros vs. Contexto) es la vacuna m√°s efectiva contra el *hype*.

* Sabemos que no "razona" como nosotros; **calcula** probabilidades.
* Sabemos que no "aprende" en tiempo real (Inferencia); solo usa su **ventana de contexto**.
* Sabemos que no es "gratis"; tiene un costo f√≠sico de **c√≥mputo y energ√≠a**.

Como "Arquitectos de IA", este conocimiento t√©cnico nos da poder. Dejamos de tratar a la IA como a un colega humano misterioso y empezamos a tratarla como lo que realmente es: un motor de alto rendimiento que requiere ingenier√≠a, mantenimiento y, sobre todo, un operador competente al volante.

Ahora que entendemos el motor, estamos listos para aprender a darle instrucciones en la **Gu√≠a 02: Ingenier√≠a de Prompts**.
